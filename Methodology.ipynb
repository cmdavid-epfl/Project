{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Story Outline\n",
    "\n",
    "## Part 1. The Big Picture\n",
    "\n",
    "In this first part, we answer the following questions:\n",
    "- What does the typical patent-holder look like today, and how has that evolved between today and the 1990's?\n",
    "- Is a migration of innovators through time visible in the data, e.g. a convergence towards certain innovation centers?\n",
    "- How has the number of assignees and inventors evolved in this period?\n",
    "\n",
    "The goal here is to address the main issue that brought us to chose this subject for our data story, by clearly showing how innovation today transcends geographies.\n",
    "\n",
    "### Approach\n",
    "\n",
    "This first part focuses on how data evolves through time from a geographic standpoint, and we decided that using maps to visualize this evolution would be the best approach. As we have access to clean longitude and latitude data (see the below section **Data Gathering**), using the **[Folium](https://python-visualization.github.io/folium/docs-v0.6.0/)** library, which has e.g. cluster-functionality using it's heat map plugin, was found to be the best way to quantify the magnitude of the networks in different geographic zones. One challenge was to show the evolution throughout the entire time period in an intuitive way. While we first thought about using Folium's layer control so that the user could manually toggle between years, for the purposes of telling a data story, we judged that it would make the story more fluid to use a timelapse. The method we used to realize this is detailed in the below section **Visualizations**.  \n",
    "\n",
    "A few time series were also plotted to provide a more quantitative perspective on the data, to be more certain about the conclusions we drew from the figures.\n",
    "    \n",
    "\n",
    "## Part 2. Peeling back the layers\n",
    "\n",
    "In this second part, we look at the following examples of innovation networks for specific patents held by the following specific assignees:\n",
    "- companies : \n",
    "  - patents :\n",
    "- academic institutions :\n",
    "  - patents :\n",
    "- governments :\n",
    "  - patents :\n",
    "\n",
    "By looking at these networks, we answer the following questions:\n",
    "- If we take a few different types of companies / government bodies / academic institutions, and look at the network supporting some of their patents, what do these networks look like, in light of the conclusions from Part 1?\n",
    "- Within the same companies, how have their networks evolved between today and the 1990's, if we look at patents similar to those above?\n",
    "- Across companies of the same type, to what extent, if any, will their networks be similar?\n",
    "\n",
    "### Approach\n",
    "\n",
    "We came to the conclusion that the most natural approach again here was to explore the data using maps to visualize the innovation networks. The specific patents which are the starting points for this analysis were chosen manually, using the **[Google Patents](https://patents.google.com/)** database to look for adequate patents to compare. We again made use of the the **Folium** library, though here we thought that the layer control would be more interesting, as we would only be looking at the 4-5 layers of patent citations supporting the chosen patents.\n",
    "\n",
    "We thought about representing the networks using a graph-based approach, using e.g. the Python library **[networkx](https://networkx.github.io/)**, though we felt like this was not adding more information than looking at the distribution of the inventor locations using **Folium**'s heat map plugin.\n",
    "\n",
    "For both parts 1 and 2, while the **data gathering** and **data preprocessing** were done mostly in parallel and in an interative fashion, for simplicity we nonetheless start by presenting the data gathering part and we then present the data preprocessing, both as in their final iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatentsView Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database offers a wide range of features for all patents since 1976, which can be extracted through their API. \n",
    "\n",
    "The most relevant documentation can be found here:\n",
    "- [Query Language Documentation](http://www.patentsview.org/api/query-language.html)\n",
    "- [Field List](http://www.patentsview.org/api/patent.html)\n",
    "\n",
    "For each research question, we came up with a list of required output **fields** which the API calls needed to return, as well as a list of input **filters** that limit the amount of extra pre-processing we had to do, while at the same time providing us with all the information necessary for us to cover the topics listed in the first section.\n",
    "\n",
    "The complete list of **fields** is as follows:\n",
    "- **Part 1**. For each patent in a given timeframe, we need:\n",
    "  - `cited_patent_number`: Patent number of the cited patents.\n",
    "  - `inventor_latitude`: Latitude of all the inventors as listed on the selected patent.\n",
    "  - `inventor_longitude`: Longitude of all the inventors as listed on the selected patent.\n",
    "  - `patent_type`: Category of patent (see below).\n",
    "  - `assignee_organization`: Organization name, if assignee is organization.\n",
    "  - `assignee_type`: Classification of assignee (see below).\n",
    "\n",
    "- For **Part 2**, the list is the same, with the exception of the last two items, which are not needed for the second part. \n",
    "\n",
    "The `assignee_type` field allows us to have a good picture of the typical patent rights holder. The `assignee_type` categories are as follows:\n",
    "- '2': US company or corporation\n",
    "- '3': foreign company or corporation\n",
    "- '4': US individual\n",
    "- '5': foreign individual\n",
    "- '6': US government\n",
    "- '7': foreign government\n",
    "- '8': country government\n",
    "- '9': US state governement\n",
    "- '1x': part interest\n",
    "\n",
    "As well, the reason for including the `patent_type` field is that we want to be able to distinguish between major patent categories:\n",
    "- 'Defensive Publication': \"... an intellectual property strategy used to prevent another party from obtaining a patent on a product, apparatus or method for instance.\" ( [wikipedia](https://en.wikipedia.org/wiki/Defensive_publication) )\n",
    "- 'Design': \"... legal protection granted to the ornamental design of a functional item\" ( [wikipedia](https://en.wikipedia.org/wiki/Design_patent) )\n",
    "- 'Plant': covering any \"new variety of plant\" ( [wikipedia](https://en.wikipedia.org/wiki/Plant_breeders'_rights) )\n",
    "- 'Reissue': correction of \"a significant error in an already issued patent\" ( [uslegal](https://definitions.uslegal.com/r/reissue-patent/) )\n",
    "- 'Statutory Invention Registration': \"for publishing patent applications on which they no longer felt they could get patents\" ( [wikipedia](https://en.wikipedia.org/wiki/United_States_Statutory_Invention_Registration) )\n",
    "- 'Utility': patent for a \"useful\" patent ( [wikipedia](https://en.wikipedia.org/wiki/Utility_(patent)) )\n",
    "\n",
    "**Utility patents** are the most relevant for us. They are patents protecting \"any new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof\" ( [U.S. Code ยง 101](https://www.law.cornell.edu/uscode/text/35/101) ) We do chose to exclude the **reissue** patent type from our results, as they are not truly innovations, but only corrections on already issued patents. This category is thus omitted directly when calling the API.\n",
    "\n",
    "We chose to query for the **latitudes** and **longitudes** instead of the location in the format city-state-country, as the former data are more useful for visualization purposes. Furthermore, during our preliminary data analysis, the data in the city-state-country format was not uniform (i.e., some cities were named in full, while others were abbreviated in different ways). On the other hand, the latitudes and longitudes data is uniform, and the missing data was also easier to clean than with names format.\n",
    "\n",
    "We also needed to be able to query for patents based on the following **filters**:\n",
    "- `patent_number`: US Patent number, as assigned by USPTO.\n",
    "- `app_date`: Date the patent application was filed (filing date)\n",
    "\n",
    "For the purposes of situating in time the patent data we have, we chose to consider the date at which the patent application was filed, instead of the date at which it was granted. The reasoning behind this choice is that at the time of the patent application, the innovation supporting it already exists. So while using the application date has reduced the amount of most-recent data (post 2014) we can work with, in our opinion it paints a more vivid picture of innovation. Furthermore, as our analysis is performed on data spanning a almost 3 decades, we're confident this choice is not detremental to our story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the PatentsView API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The maximum number of results per query through the PatentsView API is 10,000 per page, capped at 10 pages.** (In order to avoid hitting the limit, as a rule of thumb, if we are querying for all patent applications in a given date range, we need to keep the date range to about quarter of a year.) So while the PatentsView API made it easy to extract data, the usage limits meant we had to implement some functions to automate the incremental extraction and saving of all the data we needed.\n",
    "\n",
    "The following implemented functions can be found in the **[pipeline.py](https://github.com/cmdavid-epfl/Project/blob/master/pipeline.py)** module.\n",
    "\n",
    "- **patentsviewAPI**: puts together the query string, the output fields string and the options string, and then extracts and saves the data returned by the PatentsView API in json format. The following functions are called by **patentsviewAPI**:\n",
    "  - **query**: Forms query filters string to pass into **get_data**.\n",
    "  - **get_data** : Extract and save data from the PatentsView API.\n",
    "  \n",
    "The saved json data is of the following format:\n",
    "\n",
    "- page number in format '1', '2', ...\n",
    "  - 'patents': list of patents in page. For each patent:\n",
    "    - 'patent_type'\n",
    "    - 'inventors' : list of inventors listed for the patent. For each inventor:\n",
    "      - 'inventor_key_id'\n",
    "      - 'inventor_latitude'\n",
    "      - 'inventor_longitude'\n",
    "    - 'assignees': list of assignees listed for the patent. For each assignee:\n",
    "      - 'assignee_key_id'\n",
    "      - 'assignee_organization'\n",
    "      - 'assignee_type'\n",
    "    - 'cited_patents' : list of other patents cited by the current patent. For each cited patent:\n",
    "      - 'cited_patent_number'\n",
    "  - 'count': number of results in the page\n",
    "  - 'total_patent_count': total number of patents referenced in the results  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.\n",
    "\n",
    "The following functions in the **pipeline** module are specifically for preprocessing the data required for Part 1:\n",
    "- **load_data**: converts the saved jsondata from the PatentsView API to the format that is most useful in answering the research topics stated in Part 1, by calling on the following functions:\n",
    "  - **get_full_year_data**: Fetches data for a full year, one quarter at a time, to deal with the PatentsView limits.\n",
    "  - **preprocess_data**: Preprocesses saved json data from file to the format used for the data analysis.\n",
    "  \n",
    "- **get_ts**: Extracts time series data from the dataset.\n",
    "\n",
    "The preprocessed data (output of **load_data**) has the following structure:\n",
    "- for each YEAR\n",
    "  - dataframe: proportion of patents in each patent type.\n",
    "  - dataframe: unique list of all inventor locations. For each inventor location : the count of the number of inventors listed in patent applications that were based at that location at the time of the application.\n",
    "  - int: total number of patent applications.\n",
    "  - int: total number of citations made in all the patent applications.\n",
    "  - int: total number of inventors listed in the patent applications.\n",
    "  - dataframe: information on assignees. For each unique assignee:\n",
    "    - name of the assignee, if an organization\n",
    "    - type of the assignee (see **PatentsView Database** section, above)\n",
    "    - counter containing the number of inventors based in each location\n",
    "    - number of patent applications\n",
    "    - number of citations made in patent applications\n",
    "\n",
    "The extracted time series are as follows:\n",
    "- Number of patent applications for each year\n",
    "- Number of inventors listed in patent applications in each year\n",
    "- Number of patents cited in patent applications filed in each year\n",
    "- Number of utility patent applications in each year\n",
    "- Number of design patent applications in each year\n",
    "- Number of individuals listed as assignees in patent applications in each year\n",
    "\n",
    "### Missing Data and Inconsistencies\n",
    "\n",
    "The **preprocess_data** function takes care of the missing data and inconsistencies that were found during the data exploration phase. These particularities are summarized as follows.\n",
    "\n",
    "- We discard the data belonging to assignees which have no `assignee_id` attributed. \n",
    "- We discard the data with NaN as `assignee_type`. \n",
    "- We do not count the cited patents which have no corresponding patent_number, which are a negligeable amount.\n",
    "- We discard data which have either None or '0.1' in either `inventor_latitude` or `inventor_longitude`, as this data is useless for the visualizations.\n",
    "\n",
    "We show below that the amount is negligeable, as compared to the total amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required imports for loading the data for Part 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import load_data, get_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define local data folder and data year range.** Here we load the full dataset. As the data is already all on disk, the data preprocessing only takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_PATH = '/media/dcm/HDD/ADA_DATA'\n",
    "MIN_YEAR = 1990\n",
    "MAX_YEAR = 2016\n",
    "year_range = range(MIN_YEAR,MAX_YEAR + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running this next line for the full time range (1990-2016), if no data is yet saved to disk, will take a few hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/dcm/HDD/ADA_DATA 2013q1\n",
      "already on file\n",
      "/media/dcm/HDD/ADA_DATA 2013q2\n",
      "already on file\n",
      "/media/dcm/HDD/ADA_DATA 2013q3\n",
      "already on file\n",
      "/media/dcm/HDD/ADA_DATA 2013q4\n",
      "already on file\n",
      "loading data from disk\n",
      "/media/dcm/HDD/ADA_DATA/2013q1.json\n",
      "/media/dcm/HDD/ADA_DATA/2013q2.json\n",
      "/media/dcm/HDD/ADA_DATA/2013q3.json\n",
      "/media/dcm/HDD/ADA_DATA/2013q4.json\n"
     ]
    }
   ],
   "source": [
    "full_year_data = load_data(year_range, MY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_year_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_data = 0\n",
    "discarded_citations = 0\n",
    "\n",
    "for year in year_range:\n",
    "    discarded = full_year_data[year]['discarded']\n",
    "    discarded_data += discarded[0]\n",
    "    discarded_citations += discarded[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.\n",
    "\n",
    "The following functions in the **pipeline** module are specifically for preprocessing the data required for Part 2:\n",
    "- load_layers_data: Loads data from disk and converts the required data from json to dataframe.\n",
    "  - get_layers_data :\n",
    "    - get_cited_patents_data :\n",
    "    - preprocess_layer_data :\n",
    "    \n",
    "The preprocessed data (output of **load_layers_data**) has the following structure:\n",
    "- for each LAYER:\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import load_layers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_example_data = load_layers_data(filename = 'Apple', patent_number = ['9430098'], layers = 4, data_dir = MY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_example_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizations import get_png, get_timeseries_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole World\n",
    "for year in year_range:\n",
    "    get_png(full_year_data,'All',year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asia\n",
    "for year in year_range:\n",
    "    get_png(full_year_data,'All',year, zoom_on = (25,120,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Europe\n",
    "for year in year_range:\n",
    "    get_png(full_year_data,'All',year, zoom_on = (53,18,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US\n",
    "for year in year_range:\n",
    "    get_png(full_year_data,'All',year, zoom_on = (40,-90,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Assignees US, Non-US\n",
    "num_inventors_top10_us = []\n",
    "num_inventors_top10_nonus = []\n",
    "\n",
    "for year in year_range:\n",
    "    num_inventors_top10_us.append(get_png(full_year_data,'US Assignees',year, k = 10))\n",
    "    num_inventors_top10_nonus.append(get_png(full_year_data,'Non-US Assignees'),year, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series figure\n",
    "get_timeseries_fig(full_year_data, year_range, np.array(num_inventors_top10_us) / 1000, \n",
    "                   np.array(num_inventors_top10_nonus) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top assignees tables\n",
    "text_file = open(\"top_us_table.html\", \"w\")\n",
    "text_file.write(pd.read_csv(MY_PATH + '/Results/Top10_US.csv', index_col=0).to_html(index = False, \n",
    "                                                                                    col_space=200, \n",
    "                                                                                    justify = 'left'))\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(\"top_nonus_table.html\", \"w\")\n",
    "text_file.write(pd.read_csv(MY_PATH + '/Results/Top10_NonUS.csv', index_col=0).to_html(index = False, \n",
    "                                                                                    col_space=200, \n",
    "                                                                                    justify = 'left'))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizations import save_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_layers(apple_example_data, 'apple', zoom_on = None, layered = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
